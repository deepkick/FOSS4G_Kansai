{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"Data_augmentation.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kfptXLbOmumg","colab_type":"text"},"source":["# Contents\n","## Basics of PyTorch\n","- Defining network architecture\n","- Dataset and Dataloader\n","- Training\n","- Inference\n","\n","## Data Augmentation\n","- Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"XmHL2i7Nmumh","colab_type":"text"},"source":["## Massachusetts Buildings Dataset\n","In this tutorial, we use the dataset for building detection."]},{"cell_type":"code","metadata":{"id":"ol1uPH6Fm0Jn","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEPtf2RXm-LN","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/drive/My Drive/FOSS4G_Kansai/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KE_G79K-mumh","colab_type":"code","colab":{}},"source":["import numpy as np\n","from PIL import Image  as  ImgPIL\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","fpath_image = './dataset/building_vmnih/test/image/22828930_15.tiff'\n","fpath_label = './dataset/building_vmnih/test/label/22828930_15.tif'\n","\n","image = np.array(ImgPIL.open(fpath_image))\n","label = np.array(ImgPIL.open(fpath_label))\n","\n","fig = plt.figure(figsize=(20,10))\n","ax = fig.add_subplot(121)\n","ax.imshow(image, interpolation='none')\n","ax.set_xticks([])\n","ax.set_yticks([])\n","fig.show()\n","ax.set_title('Source image')\n","\n","ax = fig.add_subplot(122)\n","ax.imshow(label, interpolation='none')\n","ax.set_xticks([])\n","ax.set_yticks([])\n","ax.set_title('Ground truth')\n","\n","fig.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vLnHtByQmumk","colab_type":"text"},"source":["In the dataset, 137 pairs of images and labels like above are provided for training. Before training models on the dataset, we explain some basics of pytorch."]},{"cell_type":"markdown","metadata":{"id":"ae3cBXkLmumk","colab_type":"text"},"source":["##  Basics of PyTorch\n","PyTorch is a famous deep learning framework developed by Facebook. Here, we introduce the basics of PyTorch."]},{"cell_type":"markdown","metadata":{"id":"dq_BQdKQmuml","colab_type":"text"},"source":["Now, import pytorch modules. <br>\n","`torch.nn` contains classes for various kinds of operations such as convolution, pooling, batchnormalization, etc.<br>\n","`torch.nn.functional` contains functions for basic (and non-parametric) operations such as activation functions."]},{"cell_type":"code","metadata":{"id":"tQGSroOUmumm","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o8S4rmXZmumq","colab_type":"text"},"source":["### Tensors in PyTorch\n","We can make tensors in many ways. Tensors can be used like numpy array.\n","```\n","torch.FloatTensor([1,2,3])\n","torch.from_numpy(ndarray)\n","torch.Tensor([1,2,3]).float()\n","```\n","\n","### Basic operations\n","Before implementing network, we briefly check how to define basic operations like convolution or pooling. <br><br>\n","\n","**Convolutional layer**<br>\n","convolutional layer is implemented as a class (`nn.Conv2d`). First create an instance of the layer, then use it like a function.\n","```\n","conv1 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n","output = conv1(input)\n","```\n","\n","**Pooling layer**\n","```\n","pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n","output = pool1(input)\n","\n","pool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n","output = pool2(input)\n","```\n","\n","**Fully connected layer**\n","```\n","fc1 = torch.nn.Linear(in_features=128, out_features=128)\n","output = fc1(input)\n","```\n","\n","**Activation functions**<br>\n","Activations are implemented as functions.\n","```\n","output = torch.nn.functional.relu(input)\n","```\n","We can also use class version.\n","```\n","act1 = torch.nn.ReLU()\n","output = act1(input)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WGsV7e1Wmumr","colab_type":"text"},"source":["### Defining network architecture\n","Using the basic operations mentioned above, we implement the VGG-like model which has $64\\times64$ patches as inputs and output the estimation of center area of size $16\\times16$."]},{"cell_type":"code","metadata":{"id":"DvUf5s5amumr","colab_type":"code","colab":{}},"source":["\n","class VGGS(nn.Module):\n","    def __init__(self):\n","        super(VGGS, self).__init__()\n","        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n","        self.pool1   = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n","        self.pool2   = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n","        self.pool3   = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv4_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n","        self.conv4_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n","        self.pool4   = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc6 = nn.Linear(in_features=4096, out_features=1024)\n","        self.fc7 = nn.Linear(in_features=1024, out_features=1024)\n","        self.fc8 = nn.Linear(in_features=1024, out_features=256)\n","        \n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1_1(x))\n","        x = F.relu(self.conv1_2(x))\n","        x = self.pool1(x)\n","        x = F.relu(self.conv2_1(x))\n","        x = F.relu(self.conv2_2(x))\n","        x = self.pool2(x)\n","        x = F.relu(self.conv3_1(x))\n","        x = F.relu(self.conv3_2(x))\n","        x = self.pool3(x)\n","        x = F.relu(self.conv4_1(x))\n","        x = F.relu(self.conv4_2(x))\n","        x = self.pool4(x)\n","\n","        batch_size = x.size(0)\n","        x = x.view(batch_size, -1)\n","        \n","        x = F.dropout(self.fc6(x))\n","        x = F.dropout(self.fc7(x))\n","        x = self.fc8(x)\n","        x = x.view(batch_size, 16, 16)\n","        return x\n","    \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrfcR5B8mumt","colab_type":"text"},"source":["Create model, and check the architecture"]},{"cell_type":"code","metadata":{"id":"NNJKR4ZOmumu","colab_type":"code","colab":{}},"source":["model = VGGS()\n","print(model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T42M98B0mumw","colab_type":"text"},"source":["Check if the architecture is correct by processing random tensors."]},{"cell_type":"code","metadata":{"id":"BqTo5Enqmumx","colab_type":"code","colab":{}},"source":["inputs = np.random.randn(100,3,64,64).astype(np.float32)\n","inputs = torch.from_numpy(inputs)\n","output = model(inputs)\n","print(output.size())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iS8vvuaSmumy","colab_type":"text"},"source":["### Loss function\n","Here, we define cross entropy loss function for training. We need a small value `eps` to avoid overflow at `log()`."]},{"cell_type":"code","metadata":{"id":"JUPH75L1mumz","colab_type":"code","colab":{}},"source":["import torch.nn.functional as F\n","\n","class Criterion(nn.Module):\n","    def __init__(self):\n","        super(Criterion, self).__init__()\n","        self.eps = 1.0e-7\n","        \n","    def forward(self, inputs, targets):\n","        prob = F.sigmoid(inputs)\n","        loss = -targets.float()*(prob + self.eps).log() - (1-targets.float())*(1 - prob + self.eps).log()\n","        loss = loss.mean()\n","        return loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_FyAlWFPmum1","colab_type":"text"},"source":["## Dataset and Dataloader\n","Bellow figure presents how mini-batches for training are created in PyTorch. <br>\n","**DataLoader**: DataLoader picks up data from Dataset and pack the collected data into mini-batch. <br>\n","**Dataset**: Dataset load and pre-process data requested from DataLoader. In some cases, we need to implement the Dataset custamized for our own data. <br>"]},{"cell_type":"code","metadata":{"id":"XV0KgTsJnhzF","colab_type":"code","colab":{}},"source":["from IPython.display import Image\n","Image('fig/pth_dataset.png',  height=500)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BRgdLCNkmum2","colab_type":"text"},"source":["### Define our own Dataset"]},{"cell_type":"code","metadata":{"id":"J2Afvt8Ymum3","colab_type":"code","colab":{}},"source":["import numpy as np\n","from torch.utils.data import Dataset\n","\n","class Buildings(Dataset):\n","    def __init__(self, fpath_image_npy, fpath_label_npy):\n","        self.images = np.load(fpath_image_npy)\n","        self.labels = np.load(fpath_label_npy)\n","        \n","    def __getitem__(self, index):\n","        image = self.images[index]\n","        label = self.labels[index]\n","        \n","        image = image.transpose([2,0,1])    # (H,W,B) to (B,H,W)\n","        \n","        image_tensor = torch.from_numpy(image).float()\n","        label_tensor = torch.from_numpy(label).long()\n","        \n","        return image_tensor, label_tensor\n","    \n","    def __len__(self):\n","        return len(self.images)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_xJqrqkmum5","colab_type":"text"},"source":["Load the building dataset"]},{"cell_type":"code","metadata":{"id":"5nTKxBBFmum5","colab_type":"code","colab":{}},"source":["dataset = Buildings('./dataset/building_vmnih/train/patches/sat.npy', './dataset/building_vmnih/train/patches/map.npy')\n","image_tensor, label_tensor = dataset[0]\n","print(image_tensor.size())\n","print(label_tensor.size())\n","print(len(dataset))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qUgfp2demum8","colab_type":"text"},"source":["Then set DataLoader and check if it works. <br><br>\n","Arguments of init for DataLoader: <br>\n","batch_size: specify batch_size<br>\n","shuffle: if True, the data is randomly sampled from the Dataset and if False, the data is sequentially sampled.<br>\n","num_workers: number fo multi-thread workers. We can load data in parallel which will speed up dataloading."]},{"cell_type":"code","metadata":{"id":"wMeth-FDmum8","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader\n","\n","loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=0)\n","for image, label in loader:\n","    print(image.size(), torch.mean(image).item())\n","    print(label.size(), label[0,0,0].item())\n","    print('')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GlTRko25munA","colab_type":"text"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"mhW0bxnImunA","colab_type":"text"},"source":["Now, we prepared the training dataloader. In the following we briefly demonstrate training procedure. Since the full training takes much time, we just demonstrate first several iterations."]},{"cell_type":"markdown","metadata":{"id":"DLCHF_HPmunD","colab_type":"text"},"source":["Set maximum epoch to train. For demonstration, we set the max_epochs 1, however we recommend to use tens of epochs."]},{"cell_type":"code","metadata":{"id":"CXN5kv4jmunE","colab_type":"code","colab":{}},"source":["max_epochs = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HLfjqXS5munJ","colab_type":"text"},"source":["Transfer model to the device"]},{"cell_type":"code","metadata":{"id":"GvrJxsqxAZvo","colab_type":"code","colab":{}},"source":["use_cuda = False\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAmC7c_pmunK","colab_type":"code","colab":{}},"source":["model = model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZhNYV0mmunM","colab_type":"text"},"source":["Set optimizer. Here, we use Adam."]},{"cell_type":"code","metadata":{"id":"kRPSzSRGmunM","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.999), weight_decay=1.0e-4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2EvDCtPYmunP","colab_type":"text"},"source":["Set loss function, and transfer it to the device."]},{"cell_type":"code","metadata":{"id":"Mp7lrGDBmunQ","colab_type":"code","colab":{}},"source":["criterion = Criterion().to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1a_hSwmFmunS","colab_type":"text"},"source":["Training iterations. We demonstrate only several iterations."]},{"cell_type":"code","metadata":{"id":"sd2yXvRqmunT","colab_type":"code","colab":{}},"source":["for epoch in range(max_epochs):\n","    for idx, (image, label) in enumerate(loader):\n","        image, label = image.to(device), label.to(device)\n","        \n","        # At the begining of each iteraton, clear accumulated gradient for the network parameters.\n","        optimizer.zero_grad()\n","        \n","        # Input the image to the model and get the prediction.\n","        output = model(image)\n","        \n","        # Calculate the loss function comparing the prediction and the ground truth\n","        loss = criterion(output, label)\n","        \n","        # Backpropagate the error signal through the model to calculate gradients.\n","        loss.backward()\n","        \n","        # Update the model parameters using the calculated gradients.\n","        optimizer.step()\n","\n","        print('Epoch #%d, batch #%d: loss=%f' % (epoch, idx, loss.item()))\n","        \n","        # For demonstration, stop iteration at iter5\n","        if idx == 4:\n","            break\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LK25KEhlmunV","colab_type":"text"},"source":["### Save the learned model"]},{"cell_type":"markdown","metadata":{"id":"knvHsh0_munV","colab_type":"text"},"source":["Set output directory"]},{"cell_type":"code","metadata":{"id":"mftU8vKtmunW","colab_type":"code","colab":{}},"source":["import os\n","output_dir = './learned_weights/'\n","\n","if not os.path.isdir(output_dir):\n","    os.mkdir(output_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5H4LbfPmunX","colab_type":"text"},"source":["Get the learned weights as dictionary."]},{"cell_type":"code","metadata":{"id":"BvOIum1VmunY","colab_type":"code","colab":{}},"source":["dict_weights = model.state_dict()\n","for key, weight in dict_weights.items():\n","    print('%s\\t%s' % (key, weight.size()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yvMCFqzYmuna","colab_type":"text"},"source":["Save the dictionary."]},{"cell_type":"code","metadata":{"id":"QskJprCImuna","colab_type":"code","colab":{}},"source":["torch.save(dict_weights, output_dir + '/demo_building_vggs.torch')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSXffO30munc","colab_type":"text"},"source":["### Inference"]},{"cell_type":"markdown","metadata":{"id":"HPz5DQRrmunc","colab_type":"text"},"source":["Settings. We will crop the test image by sliding the cropping window by stride of 16."]},{"cell_type":"code","metadata":{"id":"IBcnSRIumund","colab_type":"code","colab":{}},"source":["patch_size = 64\n","aoi_size = 16\n","stride = 16"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3GPEC3KsUxI","colab_type":"code","colab":{}},"source":["Image('fig/fig_test_patch_small.png', width=450)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4yX2kTAmunf","colab_type":"text"},"source":["Prepare test area image"]},{"cell_type":"code","metadata":{"id":"dVgpL8Cvmunf","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import numpy as np\n","\n","use_cuda = False\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","from PIL import Image as ImgPIL\n","# Load image\n","fpath_test_image = './dataset/building_vmnih/test/image/22828930_15.tiff'\n","image = np.array(ImgPIL.open(fpath_test_image))\n","org_height, org_width, _ = image.shape\n","\n","# Normalize\n","mean = np.mean(image, axis=(0,1), keepdims=True)\n","std = np.std(image, axis=(0,1), keepdims=True)\n","image = (image - mean) / std\n","\n","# To tensor\n","image = image.transpose([2,0,1])    # Swap dimensions, (H,W,B) to (B,H,W)\n","image = image[np.newaxis,:,:,:]    # Add batch dimension (N,B,H,W)\n","image = torch.from_numpy(image).float()    # Convert to float tensor"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PCanWA_2muni","colab_type":"text"},"source":["Because the network outputs the estimation results of the center area of the input patches, to estimate the edge area of the image, a part of the input patch lies out of valid image region. Therefore we need expand the original image using some padding."]},{"cell_type":"code","metadata":{"id":"JThhUXLFsmL0","colab_type":"code","colab":{}},"source":["Image('fig/fig_test_padding_small.png', width=600)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"M9ToyDVvmunj","colab_type":"code","colab":{}},"source":["# Reflection padding\n","pad_size = int((64-16)/2)\n","pad = nn.ReflectionPad2d(pad_size)\n","image = pad(image)\n","_, _, height, width = image.size()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aCRp41WAmunk","colab_type":"text"},"source":["Set model and load the trained weights"]},{"cell_type":"code","metadata":{"id":"0N7zyfY9munl","colab_type":"code","colab":{}},"source":["# Set model\n","model = VGGS().to(device)\n","\n","# Load learned weights\n","learned_weights = torch.load('./learned_weights/demo_building_vggs.torch')\n","model.load_state_dict(learned_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XCQ970xBmuno","colab_type":"code","colab":{}},"source":["\n","# Set place holder\n","prob_map = np.zeros([height, width])\n","count_map = np.zeros([height, width])\n","\n","# Crop patches in sliding manner, and apply the prediction model\n","num_tiles_x = (width - patch_size) // stride + 2\n","num_tiles_y = (height - patch_size) // stride + 2\n","\n","for iy in range(num_tiles_y):\n","    for ix in range(num_tiles_x):\n","        print('(%d/%d, %d/%d)' % (iy, num_tiles_y, ix, num_tiles_x))\n","        ulx = ix * stride\n","        uly = iy * stride\n","        lrx = ulx + patch_size\n","        lry = uly + patch_size\n","        \n","        if lrx > width:\n","            ulx = width - patch_size\n","            lrx = width\n","            \n","        if lry > height:\n","            uly = height - patch_size\n","            lry = height\n","            \n","        patch = image[:, :, uly:lry, ulx:lrx]\n","        patch = patch.to(device)\n","        \n","        logit = model(patch)\n","        prob = F.sigmoid(logit)\n","        \n","        stx = ulx + int((patch_size - aoi_size) / 2)\n","        sty = uly + int((patch_size - aoi_size) / 2)\n","        prob_map[sty:sty+aoi_size, stx:stx+aoi_size] += prob.detach().cpu().numpy().squeeze()\n","        count_map[sty:sty+aoi_size, stx:stx+aoi_size] += np.ones([aoi_size,aoi_size])\n","\n","# Eliminate padded region\n","prob_map = prob_map[pad_size:pad_size+org_height, pad_size:pad_size+org_width]\n","count_map = count_map[pad_size:pad_size+org_height, pad_size:pad_size+org_width]\n","\n","# Take average for overlaped regions\n","result = prob_map / count_map\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u9ptkK5Umunp","colab_type":"text"},"source":["Check the estimation result"]},{"cell_type":"code","metadata":{"id":"2X56pr27munq","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","fpath_test_label = './dataset/building_vmnih/test/label/22828930_15.tif'\n","label = np.array(ImgPIL.open(fpath_test_label))[:,:,0]\n","\n","fig = plt.figure(figsize=(20,10))\n","ax = fig.add_subplot(121)\n","ax.imshow(result, interpolation='none')\n","ax.set_xticks([])\n","ax.set_yticks([])\n","fig.show()\n","ax.set_title('Estimation')\n","\n","ax = fig.add_subplot(122)\n","ax.imshow(label, interpolation='none')\n","ax.set_xticks([])\n","ax.set_yticks([])\n","ax.set_title('Ground truth')\n","\n","fig.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuGaC-pjmunr","colab_type":"code","colab":{}},"source":["import numpy as np\n","pred = (result > 0.5)\n","label = (label > 0.5)\n","\n","TP = np.sum(pred*label)\n","T = np.sum(label)\n","P = np.sum(pred)\n","IoU = float(TP) / (T+P-TP) * 100\n","print('IoU: %.2f%%' % IoU)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZZxPImimunt","colab_type":"text"},"source":["Next, we add data augmentation (random rotation and flipping) to our Dataset class."]},{"cell_type":"code","metadata":{"id":"4kxkArUzmunu","colab_type":"code","colab":{}},"source":["import numpy as np\n","from torch.utils.data import Dataset\n","import random\n","from PIL import Image as ImgPIL\n","\n","class Buildings(Dataset):\n","    def __init__(self, fpath_image_npy, fpath_label_npy, augmentation=False):\n","        self.images = np.load(fpath_image_npy)\n","        self.labels = np.load(fpath_label_npy)\n","        self.augmentation = augmentation\n","        \n","    def __getitem__(self, index):\n","        image = self.images[index]\n","        label = self.labels[index]\n","        \n","        # Augmentation\n","        if self.augmentation:\n","            # Randomly choose rotation angle and flipping direction\n","            rotation = random.choice([0, 90, 180, 270])\n","            flip = random.choice(['H', 'V', 'N'])\n","            \n","            # Apply transformation for image\n","            image = self._rotate(image, rotation)\n","            image = np.array(self._flip(image, flip))\n","\n","            # Also, apply the same transformation for label\n","            label = self._rotate(label, rotation)\n","            label = np.array(self._flip(label, flip))\n","        \n","        image = image.transpose([2,0,1])    # (H,W,B) to (B,H,W)\n","        \n","        image_tensor = torch.from_numpy(image).float()\n","        label_tensor = torch.from_numpy(label).long()\n","        \n","        return image_tensor, label_tensor\n","    \n","    def __len__(self):\n","        return len(self.images)\n","    \n","    # Rotation function\n","    def _rotate(self, image, rotation):\n","        if rotation == 0:\n","            return image\n","        elif rotation == 90:\n","            image = image.swapaxes(0,1)\n","            return np.flip(image, axis=0)\n","        elif rotation == 180:\n","            image = np.flip(image, axis=0)\n","            return np.flip(image, axis=1)\n","        elif rotation == 270:\n","            image = image.swapaxes(0,1)\n","            return np.flip(image, axis=1)\n","        else:\n","            raise RuntimeError\n","\n","    # Flip function\n","    def _flip(self, image, direction):\n","        if direction == 'H':\n","            return np.flip(image, axis=1)\n","        elif direction == 'V':\n","            return np.flip(image, axis=0)\n","        elif direction == 'N':\n","            return image\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"svFbxUzKmunw","colab_type":"text"},"source":["Check the augmentation by sampling some patches from the Dataset."]},{"cell_type":"code","metadata":{"id":"pp8HfIwDmunx","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","from matplotlib.gridspec import GridSpec\n","grid = GridSpec(nrows=2, ncols=10)\n","\n","\n","mean = np.array([75, 75, 75])\n","std = np.array([50, 50, 50])\n","\n","mean = mean[:, np.newaxis, np.newaxis]\n","std = std[:,np.newaxis, np.newaxis]\n","\n","dataset = Buildings('./dataset/building_vmnih/train/patches/sat.npy', './dataset/building_vmnih/train/patches/map.npy', augmentation=True)\n","\n","fig = plt.figure(figsize=(20,4))\n","for i in range(10):\n","    image_tensor, label_tensor = dataset[3]\n","    image_patch = image_tensor.detach().numpy() * std + mean\n","    label_patch = label_tensor.detach().numpy()\n","    \n","    image_patch = image_patch.transpose([1,2,0])\n","    image_patch = image_patch.clip(0,255).astype(np.uint8)\n","\n","    ax = fig.add_subplot(grid[0,i])\n","    ax.imshow(image_patch, interpolation='none')\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","\n","    ax = fig.add_subplot(grid[1,i])\n","    ax.imshow(label_patch, interpolation='none')\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","fig.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-I6mp5Smunz","colab_type":"text"},"source":["**Displaying the result from trained model using large-scale data with more epochs**"]},{"cell_type":"code","metadata":{"id":"q3-27bwpCktG","colab_type":"code","colab":{}},"source":["Image('fig/bulding_detct.png')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5oMmayHOmun0","colab_type":"text"},"source":["Such techniques are effective especially when we have limited amount of labeled data.\n","There are other methods which is \n","\n","- Transfer learning\n","- Data fusion\n"]}]}