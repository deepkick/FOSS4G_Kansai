{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"Data_preparation.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"9zRQpTB9Fb6K","colab_type":"text"},"source":["# Data preparation\n","## Source dataset\n","ISPRS Vaihingen Dataset is provided by Commission III of the ISPRS. The dataset is composed of 9 cm spatial resolution\n","aerial imagery with red and green bands. Dataset includes 16 labeled scenes which cover roughly 0.6 km2.\n","Labels are provided for 6 classes: impervious surface, building, low vegetation, tree, car and clutter/background. Usually, clutter/background class are ignored because the amount of labels are very limited.\n","## Check data"]},{"cell_type":"code","metadata":{"id":"GgQUQI3cF5av","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8JeBCPoF1nc","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/drive/My Drive/FOSS4G_Kansai/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3uZ0lejFb6L","colab_type":"code","colab":{}},"source":["import glob\n","print(glob.glob('dataset/isprs_vaihingen/train/*/'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sJx1NCqFb6O","colab_type":"code","colab":{}},"source":["print(glob.glob('dataset/isprs_vaihingen/val/*/'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"btf99rG1Fb6Q","colab_type":"code","colab":{}},"source":["print(glob.glob('dataset/isprs_vaihingen/test/*/'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PXSTix7Fb6T","colab_type":"text"},"source":["### Check training images and labels"]},{"cell_type":"code","metadata":{"id":"V36l77YkFb6T","colab_type":"code","colab":{}},"source":["from PIL import Image\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fqULChTKFb6V","colab_type":"text"},"source":["Read image as ndarray using PIL"]},{"cell_type":"code","metadata":{"id":"5AZL1T6IFb6W","colab_type":"code","colab":{}},"source":["image = np.array(Image.open('./dataset/isprs_vaihingen/train/image/top_mosaic_09cm_area21.tif'))\n","label = np.array(Image.open('./dataset/isprs_vaihingen/train/label_rgb/top_mosaic_09cm_area21.tif'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vlurXK9tFb6Y","colab_type":"code","colab":{}},"source":["print(image.shape)\n","print(label.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hQtLhfeFb6Z","colab_type":"code","colab":{}},"source":["# import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","# ignore annoying warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(121)\n","ax.imshow(image, interpolation='none')\n","ax.set_xticks([])\n","ax.set_yticks([])\n","ax.set_title('Image')\n","fig.show()\n","\n","ax = fig.add_subplot(122)\n","ax.imshow(label, interpolation='none')\n","ax.set_xticks([])\n","ax.set_yticks([])\n","ax.set_title('Label')\n","\n","fig.suptitle('Scene: top_mosaic_09cm_area1')\n","fig.show()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pNFF0bE8Fb6b","colab_type":"text"},"source":["Class labels are as follows <br>\n","0: Impervious surface (white)<br>\n","1: building (blue)<br>\n","2: low vegetation (cyan)<br>\n","3: tree (green)<br>\n","4: car (yellow)<br>"]},{"cell_type":"markdown","metadata":{"id":"FwOrqC1SFb6c","colab_type":"text"},"source":["## Pre-processing\n","In order to apply deep learning methods, we need to do some pre-processing as follows.\n","* Cropping\n","* Normalization\n","\n","## Cropping\n","Now, our training image (area1) has the size of $1919\\times2569$. As you know, it is common that remote sensing imagery have such a large size. One problem is that deep learning methods cannnot be directly applied for such large images because the methods are usually memory consuming. It is often the case that one model requires several GB RAM memory to process patches of size $256\\times256$. In order to apply deep learning to remote sensing imagery, we need to crop small patches from the large images. Here, we demonstrate how to do that."]},{"cell_type":"markdown","metadata":{"id":"XWO8rDeCFb6c","colab_type":"text"},"source":["### Crop single patch randomly\n","In the following, we randomly crop the training patches of size $256\\times256$ from training images and labels. First, we demonstrate cropping of single patch from the original large images."]},{"cell_type":"markdown","metadata":{"id":"b33xtL6qFb6d","colab_type":"text"},"source":["Set file path for the training images and the corresponding labels."]},{"cell_type":"code","metadata":{"id":"kU7nUHs6Fb6e","colab_type":"code","colab":{}},"source":["fpath_train_image1 = './dataset/isprs_vaihingen/train/image/top_mosaic_09cm_area21.tif'\n","fpath_train_image2 = './dataset/isprs_vaihingen/train/image/top_mosaic_09cm_area7.tif'\n","fpath_train_label1 = './dataset/isprs_vaihingen/train/label/top_mosaic_09cm_area21.tif'\n","fpath_train_label2 = './dataset/isprs_vaihingen/train/label/top_mosaic_09cm_area7.tif'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TZ9gS9_LFb6g","colab_type":"text"},"source":["Import modules"]},{"cell_type":"code","metadata":{"id":"U7lRD8LFFb6g","colab_type":"code","colab":{}},"source":["import glob\n","import random\n","from PIL import Image\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6tV06lxUFb6i","colab_type":"text"},"source":["Load training images and labels"]},{"cell_type":"code","metadata":{"id":"FTGvnwJSFb6j","colab_type":"code","colab":{}},"source":["image1 = np.array(Image.open(fpath_train_image1))\n","image2 = np.array(Image.open(fpath_train_image2))\n","label1 = np.array(Image.open(fpath_train_label1))\n","label2 = np.array(Image.open(fpath_train_label2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"05MYqHlXFb6l","colab_type":"text"},"source":["Arrange image and label pair into list"]},{"cell_type":"code","metadata":{"id":"ENqKLOCgFb6l","colab_type":"code","colab":{}},"source":["list_data = [\n","    ['top_mosaic_09cm_area21', image1, label1],\n","    ['top_mosaic_09cm_area7' , image2, label2],\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvHTn0Z7Fb6n","colab_type":"text"},"source":["Randomly select a target scene from the list of training scene"]},{"cell_type":"code","metadata":{"id":"hPtcAWx0Fb6n","colab_type":"code","colab":{}},"source":["name, image, label = random.choice(list_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dx76EsjtFb6p","colab_type":"text"},"source":["Select crop position randomly"]},{"cell_type":"code","metadata":{"id":"sm-MVyqvFb6q","colab_type":"code","colab":{}},"source":["height, width, _ = image.shape\n","patch_size = 256\n","ulx = random.randrange(0, width - patch_size + 1)\n","uly = random.randrange(0, height - patch_size + 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Bg0sY8LFb6r","colab_type":"text"},"source":["Crop a patch pair"]},{"cell_type":"code","metadata":{"id":"3JlaTkdpFb6s","colab_type":"code","colab":{}},"source":["image_patch = image[uly:uly+patch_size, ulx:ulx+patch_size, :]\n","label_patch = label[uly:uly+patch_size, ulx:ulx+patch_size]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvlqs59-Fb6x","colab_type":"text"},"source":["Now, we have cropped a patch pair of size $256\\times256$. Let's check if the patches are correctly cropped by showing them."]},{"cell_type":"code","metadata":{"id":"j4Li4kyOFb6y","colab_type":"code","colab":{}},"source":["fig = plt.figure()\n","ax = fig.add_subplot(121)\n","ax.imshow(image_patch, interpolation='none')\n","ax.set_xticks([])\n","ax.set_yticks([])\n","fig.show()\n","ax.set_title('Image patch')\n","\n","ax = fig.add_subplot(122)\n","ax.imshow(label_patch, interpolation='none')\n","ax.set_xticks([])\n","ax.set_yticks([])\n","ax.set_title('Label patch')\n","\n","fig.suptitle('Scene: %s at (%d,%d)-(%d,%d)' % (name, ulx, uly, ulx+patch_size, uly+patch_size))\n","fig.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3JB1A1PFb60","colab_type":"text"},"source":["Neural networks take as input the image patch on the left, and output corresponding classification result.\n","The goal of training is to optimize the network parameters so that the output of the network become close to the true label on the right.\n","In order to train networks that generalize to unseen images, we need variety of such training patch pairs so that the networks can learn variety of patterns. So, in the following, we crop multiple patches from the training scenes."]},{"cell_type":"markdown","metadata":{"id":"X1e-21YSFb61","colab_type":"text"},"source":["### Crop multiple patches randomly\n"," As stated above, here we crop multiple patches from the training scenes. Basically, we just apply the same procedure as above iteratively."]},{"cell_type":"markdown","metadata":{"id":"h0U0eMFAFb62","colab_type":"text"},"source":["Settings for the cropping"]},{"cell_type":"code","metadata":{"id":"xWMXA-nxFb63","colab_type":"code","colab":{}},"source":["num_patches = 750\n","patch_size = 256"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LSGscjjrFb68","colab_type":"text"},"source":["Define place holders for containing cropped patches"]},{"cell_type":"code","metadata":{"id":"8WOdthSyFb6-","colab_type":"code","colab":{}},"source":["list_image_patches = []\n","list_label_patches = []"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LHGOfTFFFb7B","colab_type":"text"},"source":["Iterate the random cropping untile the defined number of patches are collected."]},{"cell_type":"code","metadata":{"id":"-5YbsWt8Fb7C","colab_type":"code","colab":{}},"source":["for i in range(num_patches):\n","    # choose scenes randomly\n","    name, image, label = random.choice(list_data)\n","\n","    # choose crop position randomly\n","    height, width, _ = image.shape\n","    ulx = random.randrange(0, width - patch_size + 1)\n","    uly = random.randrange(0, height - patch_size + 1)\n","\n","    # crop\n","    image_patch = image[uly:uly+patch_size, ulx:ulx+patch_size, :]\n","    label_patch = label[uly:uly+patch_size, ulx:ulx+patch_size]\n","\n","    # contain\n","    list_image_patches.append(image_patch)\n","    list_label_patches.append(label_patch)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6jjWdWTTFb7E","colab_type":"text"},"source":["Convert place holder to ndarray"]},{"cell_type":"code","metadata":{"id":"4btg46VrFb7F","colab_type":"code","colab":{}},"source":["npy_image_patches = np.array(list_image_patches)\n","npy_label_patches = np.array(list_label_patches)\n","\n","print(npy_image_patches.shape)\n","print(npy_label_patches.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-quot0vYFb7H","colab_type":"text"},"source":["Check the cropped patch pairs"]},{"cell_type":"code","metadata":{"id":"qBXhP0H2Fb7I","colab_type":"code","colab":{}},"source":["from matplotlib.gridspec import GridSpec\n","grid = GridSpec(nrows=2, ncols=10)\n","\n","fig = plt.figure(figsize=(20,4))\n","\n","for i in range(10):\n","    idx = random.randrange(len(npy_image_patches))\n","    image_patch = npy_image_patches[idx]\n","    label_patch = npy_label_patches[idx]\n","    \n","    ax = fig.add_subplot(grid[0,i])\n","    ax.imshow(image_patch, interpolation='none')\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    ax.set_title('ID:%d' % idx)\n","\n","    ax = fig.add_subplot(grid[1,i])\n","    ax.imshow(label_patch, interpolation='none')\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","fig.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QGD0QBduFb7K","colab_type":"text"},"source":["Finally, implement the cropping code as function so that we can reuse later.<br><br>\n","**Input**: \n","- list of scenes (name, image, label)\n","- number of patches to crop\n","- size of the patches\n","\n","**Output**: \n","- cropped image patches (ndarray)\n","- cropped label patches (ndarray) "]},{"cell_type":"code","metadata":{"id":"pHvbyYxIFb7L","colab_type":"code","colab":{}},"source":["def random_crop(list_data, num_patches, patch_size):\n","    list_image_patches = []\n","    list_label_patches = []\n","\n","    for i in range(num_patches):\n","        # choose scenes randomly\n","        name, image, label = random.choice(list_data)\n","\n","        # choose crop position randomly\n","        height, width, _ = image.shape\n","        ulx = random.randrange(0, width - patch_size + 1)\n","        uly = random.randrange(0, height - patch_size + 1)\n","\n","        # crop\n","        image_patch = image[uly:uly+patch_size, ulx:ulx+patch_size, :]\n","        label_patch = label[uly:uly+patch_size, ulx:ulx+patch_size]\n","\n","        # contain\n","        list_image_patches.append(image_patch)\n","        list_label_patches.append(label_patch)\n","    \n","    npy_image_patches =  np.array(list_image_patches)\n","    npy_label_patches =  np.array(list_label_patches)\n","\n","    return npy_image_patches, npy_label_patches\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6c_oeoKFb7p","colab_type":"text"},"source":["## Normalization\n","Another pre-processing that is necessary is normalization of the cropped patches. Usually, the input images for neural networks are normalized. Simple but widely used methods is range adjustments. The methods adjust the range of pixel value to \\[0,1\\] or \\[-1,1\\]. For example, if we have a 8bit image, dividing each pixel value by 255 results in the pixel values of range \\[0,1\\]. For ground-based images, these methods are widely used. However, for remote sensing images, we recommend to normalize each band to have zero mean and unit variance. \n","\n","### Why we need normalization?\n","There are mainly two reasons.\n","1. **For fast training and good convergence**. The initialization of neural network parameters assumes normalized inputs. Therefore, without normalization, the initial parameter value will be far from optimal, which prolongs training time or even worse the training is not converge. \n","2. **For robustness**. To normalize the influence of acquisition conditions (e.g. lighting conditions or sensors).\n","\n","In the following, we demonstrate the normalization."]},{"cell_type":"markdown","metadata":{"id":"7yaY-qliFb7q","colab_type":"text"},"source":["Calculate band-wise mean and std"]},{"cell_type":"code","metadata":{"id":"zu0jdMorFb7q","colab_type":"code","colab":{}},"source":["mean1 = np.mean(image1, axis=(0,1), keepdims=True)\n","mean2 = np.mean(image2, axis=(0,1), keepdims=True)\n","std1 = np.std(image1, axis=(0,1), keepdims=True)\n","std2 = np.std(image2, axis=(0,1), keepdims=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TbT3MwVJFb7s","colab_type":"text"},"source":["Normalize original image"]},{"cell_type":"code","metadata":{"id":"dtz-H3xcFb7t","colab_type":"code","colab":{}},"source":["image1_normalized = (image1 - mean1) / std1\n","image2_normalized = (image2 - mean2) / std2\n","\n","list_data_normalized = [\n","    ['top_mosaic_09cm_area21', image1_normalized, label1],\n","    ['top_mosaic_09cm_area7' , image2_normalized, label2],\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5t5sKPxhFb7x","colab_type":"text"},"source":["Random crop using normalized images"]},{"cell_type":"code","metadata":{"id":"4dkRtGi3Fb7x","colab_type":"code","colab":{}},"source":["npy_image_patches, npy_label_patches = random_crop(list_data_normalized, num_patches=750, patch_size=256)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MymSbtEbFb7y","colab_type":"text"},"source":["Now, we have completed pre-processing for training data. Finally, let's save the cropped patches as numpy file."]},{"cell_type":"code","metadata":{"id":"g8Ka8A9WFb7z","colab_type":"code","colab":{}},"source":["#  Set output directory for cropped patches\n","import os\n","output_dir_train = 'dataset/isprs_vaihingen/train/patches/'\n","if not os.path.isdir(output_dir_train):\n","    os.mkdir(output_dir_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KboJTNiQFb70","colab_type":"text"},"source":["Save patches as numpy file"]},{"cell_type":"code","metadata":{"id":"AmBYvWXEFb71","colab_type":"code","colab":{}},"source":["np.save(output_dir_train + '/image.npy', npy_image_patches)\n","np.save(output_dir_train + '/label.npy', npy_label_patches)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJhf5qEUFb72","colab_type":"text"},"source":["### Pre-process validation patches\n","Pre-process validation patches in the same way as training patches"]},{"cell_type":"code","metadata":{"id":"pBq9iqJ5Fb73","colab_type":"code","colab":{}},"source":["# set file path\n","fpath_val_image = './dataset/isprs_vaihingen/val/image/top_mosaic_09cm_area11.tif'\n","fpath_val_label = './dataset/isprs_vaihingen/val/label/top_mosaic_09cm_area11.tif'\n","\n","# pre-load images and labels\n","image = np.array(Image.open(fpath_val_image))\n","label = np.array(Image.open(fpath_val_label))\n","\n","# normalize\n","mean = np.mean(image1, axis=(0,1), keepdims=True)\n","std = np.std(image1, axis=(0,1), keepdims=True)\n","image_normalized = (image - mean) / std\n","\n","# to list\n","list_data_normalized = [\n","    ['top_mosaic_09cm_area11', image_normalized, label],\n","]\n","\n","# cropping\n","npy_image_patches, npy_label_patches = random_crop(list_data_normalized, num_patches=100, patch_size=256)\n","\n","# save\n","output_dir_val = './dataset/isprs_vaihingen/val/patches/'\n","if not os.path.isdir(output_dir_val):\n","    os.mkdir(output_dir_val)\n","np.save(output_dir_val + '/image.npy', npy_image_patches)\n","np.save(output_dir_val + '/label.npy', npy_label_patches)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fv9oDP8mFb75","colab_type":"text"},"source":["### Pre-process test patches for prediction"]},{"cell_type":"markdown","metadata":{"id":"FrH0NmOIFb75","colab_type":"text"},"source":["Test patches are not randomly created.  "]},{"cell_type":"code","metadata":{"id":"JeMA74zUFb76","colab_type":"code","colab":{}},"source":["from PIL import Image\n","import numpy as np\n","import os\n","\n","fpath_test_image = './dataset/isprs_vaihingen/test/image/top_mosaic_09cm_area30.tif'\n","fpath_test_label = './dataset/isprs_vaihingen/test/label/top_mosaic_09cm_area30.tif'\n","image = np.array(Image.open(fpath_test_image))\n","label = np.array(Image.open(fpath_test_label))\n","\n","# Normalize\n","mean = np.mean(image, axis=(0,1), keepdims=True)\n","std = np.std(image, axis=(0,1), keepdims=True)\n","image = (image - mean) / std\n","\n","patch_size = 256\n","stride = 256\n","height, width, _ = image.shape\n","\n","num_tiles_x = (width - patch_size) // stride + 1\n","num_tiles_y = (height - patch_size) // stride + 1\n","\n","list_image_patches = []\n","list_label_patches = []\n","for iy in range(num_tiles_y):\n","    for ix in range(num_tiles_x):\n","        ulx = ix * stride\n","        uly = iy * stride\n","        lrx = ulx + patch_size\n","        lry = uly + patch_size\n","            \n","        image_patch = image[uly:lry, ulx:lrx, :]\n","        label_patch = label[uly:lry, ulx:lrx]\n","        \n","        list_image_patches.append(image_patch)\n","        list_label_patches.append(label_patch)\n","\n","npy_image_patches = np.array(list_image_patches)\n","npy_label_patches = np.array(list_label_patches)\n","\n","# save\n","output_dir_test = './dataset/isprs_vaihingen/test/patches/'\n","if not os.path.isdir(output_dir_test):\n","    os.mkdir(output_dir_test)\n","np.save(output_dir_test + '/image.npy', npy_image_patches)\n","np.save(output_dir_test + '/label.npy', npy_label_patches)\n","\n","print(npy_image_patches.shape)\n","print(npy_label_patches.shape)\n"],"execution_count":0,"outputs":[]}]}